{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2375fea-76a2-42fa-80ee-e626c4c232e8",
   "metadata": {},
   "source": [
    "# Easy OMOP ETL via the Harmonization Copilot\n",
    "The Harmonization CoPilot is a powerful tool designed to streamline the transformation of electronic health record (EHR) data into the OMOP common data model. This tutorial explores the process of harmonizing data using the Harmonization CoPilot, guiding you through the key steps involved in syntactic and semantic mapping, as well as running the data harmonization process. **In this example, we'll transform three clinical data tables into their corresponding OMOP tables: Person, Visit Occurrence, and Measurement.** \n",
    "\n",
    "## Overview\n",
    "\n",
    "Harmonizing data is a multi-step process that involves both schema transformations (aka syntactic mapping) and code transformations (aka semantic mapping).  Once this structural foundation is in place, the next step is ensuring that the data values align with standard terminologies through semantic mapping. This crucial step allows data fields, such as race, ethnicity, service type, and lab test names, to conform to OMOP’s standard codes, enhancing consistency and interoperability.\n",
    "\n",
    "1. Create a Syntactic Mapping – Map the three source tables to their corresponding OMOP tables.\n",
    "2. Create the Required Semantic Mappings – Harmonize race, ethnicity, type of service codes, and lab test names to OMOP standard codes.\n",
    "3. Run the Data Harmonization Code Object – Apply the syntactic and semantic mappings to transform the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d3347-58d2-43b3-9df8-7af0f2bed4ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1: Investigate Your Data\n",
    "The first step in any data harmonization project is to develop a deep understanding the source data that is being transformed. The following questions can serve as a guide when reviewing such data:\n",
    "\n",
    "1. ***What information am I standardizing?*** In this example, the goal is to standardize data abouth patients treated within my health system, their clinical visits (both inpatient and outpatient), and laboratory test results.\n",
    "2. ***In which data tables is the information located?*** Often multiple data warehouse tables will store the relevant data (this scenario is supported by the Copilot). However, the scenario in this notebook is straightforward because each clinical event is stored within their own table.\n",
    "3. ***In which columns is the information located?*** When transforming data to a common data model, you'll isolate relevant columns in your source data. Columns with information not required by the data model can be ignored. \n",
    "4. ***How are the columns encoded?*** Often, important information like laboratory tests and hospital procedures are represented using vocabularies that are specific to a given hospital or health system. You'll want to identify these as targets for AI-powered semantic mapping. In the case that columns are represented by standard vocabularies, performing semantic mapping via generative AI may not be necessary. \n",
    "\n",
    "Once you've developed a deep understanding of the data, [import the relevant tables onto the Rhino Federated Computing Platform](https://docs.rhinohealth.com/hc/en-us/articles/12385893636509-Creating-a-New-Dataset-or-Dataset-Version). \n",
    "\n",
    "![title](images/source_data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089e206-a522-45b1-b3a3-1e80bbe9f088",
   "metadata": {},
   "source": [
    "## Step 2: Create Syntactic Mapping\n",
    "Syntactic mapping serves as the blueprint for translating your source data into the OMOP model. It defines how each table and field will be mapped and transformed. Within the Harmonization CoPilot, users can create and manage these mappings through an intuitive interface. Following these simple steps to create a new syntactic mapping:\n",
    "1. Select **Data Mappings** on the left toolbar.\n",
    "2. Select **Syntactic Mappings** on the tab menu.\n",
    "3. Select **Create Syntactic Mapping** button.\n",
    "4. Select **OMOP V5.4** as the target data model.\n",
    "5. Select **Manually Configure**. This will allow you to use the user-interface to design your syntactic mapping. Otherwise, you can upload a JSON file that specifies the mapping. We recommend the latter only for power users!\n",
    "6. Select your **Source Data Schemas**. You'll want to select the auto-generated schemas associated with each of the source datasets you uploaded in the previous step. If your dataset is named 'My Patient Data', for example, the schema will be named 'My Patient Data (v0)'. If you want to transform 3 source datasets, you'll select 3 schemas!\n",
    "7. Select the **Target Tables** that you are transforming data into. You'll refer to the [OMOP Common Data Model documentation](https://ohdsi.github.io/CommonDataModel/cdm54.html) to identify the relevant tables to transform to. In this case, we are tranforming 'My Patient Dataset' into the OMOP 'Person' table, 'My Encounter Data' into the OMOP 'Visit Occurrence' table, and 'My Laboratory Data' into the Measurement table.\n",
    "\n",
    "![title](images/create_syntactic.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebad9e-adfa-4286-b9e9-88ff9497b7f5",
   "metadata": {},
   "source": [
    "#### Using the graphical interface to design my ETL\n",
    "Once a syntactic mapping is created, the dialog to map from source to target tables will automatically appear. \n",
    "\n",
    "The **Target Field column** lists all the columns associated with the target tables selected in the previous step. A red asterisk indicates that the target field is required - don't forget to provide mappings for these, or else you'll have trouble later! \n",
    "\n",
    "For each required target field, ***select one or more source fields*** to map to the target field. Click on the source field dropdown and you can select fields from any input data schemas for the syntactic mapping. If you select more than one field, make sure that you select fields from the same schema. \n",
    "\n",
    "In the case that the source data needs to be modified in any way to meet the requirements of the target field, you'll specify a transformation in the **Transformation column**. You can createa and edit the transformations by clicking the pencil icon ein the Transformations column. \n",
    "\n",
    "![title](images/syntactic_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921b77d-f521-42f3-89c4-eb4b81c3d218",
   "metadata": {},
   "source": [
    "#### Column Transformations: The 'Workhorse' of Data Harmonization\n",
    "\n",
    "In the Copilot, Transformations do the real work of data harmonization by modifying source data to comply with the specifications of a target data model. \n",
    "\n",
    "In mapping these three source tables to the OMOP Common Data Model, we used the following transformation types:\n",
    "\n",
    "**Custom Mapping:** Maps values from the source to corresponding values in the target dataset, like mapping a number to the day of the week. In this example, a Custom Mapping transformation was used to map the source 'Gender' field to the 'gender_concept_id' field in the OMOP Person table. The transformation was specified as a CSV and pasted into the entry box:\n",
    "\n",
    "> Male, 8251\n",
    "> \n",
    "> Female, 8329\n",
    "> \n",
    "> Other, 8391\n",
    "\n",
    "**Semantic Mapping:** Applies a semantic mapping to transform values, like mapping an input text to an OMOP concept ID. OMOP domains are helpful high-level categories that allow non-expects to select the appropriate set of standardized codes to map non-standard codes to. \n",
    "\n",
    "In this example, four semantic mappings were created:\n",
    "1. Source column *gender* to OMOP Gender domain.\n",
    "2. Source column *race* to OMOP Race domain.\n",
    "3. Source column *service_type* to OMOP Visit domain.\n",
    "4. Source column *test_name* to OMOP Measurement domain. \n",
    "\n",
    "**Set Value** Assigns a specific value to all rows in the field, like setting all values to the number 1. \n",
    "\n",
    "(*Helpful Hint:* in the case that information is missing for a required OMOP field, encode a set value of 0, which is the concept_id for 'Missing Information').\n",
    "\n",
    "**Convert Date** Changes the date to a different format.  For example, you can use this to convert a date from MM/DD/YY format to YYYY-MM-DD.\n",
    "\n",
    "**Stable UUID** Generates an encrypted unique identifier based on the input. The same input will always generate the same unique identifier.\n",
    "\n",
    "![title](images/syntactic_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ceda4-65c9-4b71-ba28-310335e20703",
   "metadata": {},
   "source": [
    "## Step 3: Execute Data Harmonization via the User Interface or SDK\n",
    "With both syntactic and semantic mappings in place, the final step is executing the data harmonization process. The Harmonization CoPilot automates this transformation, applying the defined mappings to convert raw EHR data into the OMOP standard. This can be performed either by using the user-interface of Rhino's web platform or by using Rhino's Python SDK, the latter of which enables users to build automated data pipelines.   This process takes place within the secure environment of the Rhino Client, maintaining compliance with data privacy policies.\n",
    "\n",
    "#### Execute Data Harmonization via the User Interface: Ideal for One-Time Harmonization\n",
    "To initiate the harmonization process, users select the relevant syntactic mapping, input datasets, and associated semantic mappings. The system then processes the data, applying transformations and standardizations in a structured manner. Throughout this process, users can monitor progress and review detailed logs to ensure accuracy. Once completed, the harmonized dataset is stored within the same environment, ready for further analysis or export.\n",
    "![title](images/code_run.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b3a47-e33a-4fe0-8b9b-ee04ca6ea4e0",
   "metadata": {},
   "source": [
    "#### Execute Data Harmonization via the User Interface: Ideal for Repeated Harmonization\n",
    "Users who are interested in configuring a resuable data pipeline will want to leverage the Rhino Python SDK. Once the Copilot is used to create a Data Harmonization code object, a user should develop a script that performs the following actions:\n",
    "1. Retrieves new data from an enterprise data warehouse\n",
    "2. Executes Data Harmonization Code Object(s) to generate harmonized datasets\n",
    "3. Export harmonized datasets\n",
    "\n",
    "##### Authenticate to the FCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbeb6cd3-1b07-4e45-8e19-9367d298ffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are not using the latest version of the Rhino SDK.\n",
      "Latest version: 1.4.1\n",
      "Current version: 1.3.3\n",
      "To upgrade, run: pip install --upgrade rhino_health\n"
     ]
    }
   ],
   "source": [
    "import rhino_health as rh\n",
    "from getpass import getpass\n",
    "from rhino_health.lib.endpoints.code_object.code_object_dataclass import (\n",
    "    CodeObjectCreateInput,\n",
    "    CodeTypes,\n",
    "    CodeObjectRunInput,\n",
    ")\n",
    "# Enter Rhino username and password\n",
    "my_username = \"daniel@rhinohealth.com\" # REPLACE\n",
    "session = rh.login(username=my_username, password=getpass())\n",
    "\n",
    " # Alternatively, identify project by UID\n",
    "project_uid = '845e30e1-519f-464c-935a-89705936e482' # REPLACE\n",
    "workgroup_uid = 'e590e0fa-ae37-48b3-b50e-c232536cefab' # REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5d852-18ec-4980-932e-ce6ece22f842",
   "metadata": {},
   "source": [
    "##### Retrieve new data from an enterprise data warehouse\n",
    "Rhino's SDK offers the ability to retrieve data from a relational database and seamlessly create datasets within the FCP, which are the input to the Harmonization Copilot. You can adapt the code below to your own database and Copilot project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c6941-f938-4ce3-a712-0d06e027d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define database connection details\n",
    "connection_details = ConnectionDetails(\n",
    "    server_user=\"user\", # REPLACE\n",
    "    password=getpass.getpass(),    \n",
    "    server_type=SQLServerTypes.POSTGRESQL, # see docs for all supported type\n",
    "    server_url=\"url\", # REPLACE\n",
    "    db_name=\"db_name\" # REPLACE\n",
    ")\n",
    "\n",
    "# define SQL query and other parameters\n",
    "import_run_params = SQLQueryImportInput(\n",
    "    session = session,\n",
    "    project = project_uid, \n",
    "    workgroup = workgroup_uid,\n",
    "    connection_details = connection_details,\n",
    "    cohort_name = 'dataset_name', # REPLACE\n",
    "    data_schema_uid = None, # Auto-Generating the Output Data Schema for the Cohort\n",
    "    timeout_seconds = 1200,\n",
    "    is_data_deidentified = True,\n",
    "    sql_query = \"SELECT * FROM schema.my_table\" \n",
    ")\n",
    "\n",
    "# execute SQL query and store uid of resulting dataset\n",
    "response = session.sql_query.import_cohort_from_sql_query(import_run_params)\n",
    "updated_dataset = get_dataset_by_name(name = 'dataset_name', project_uid = project_uid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2a26a-a3f0-4b9e-a6f3-f79fb6399e07",
   "metadata": {},
   "source": [
    "##### Execute Data Harmonization Code Object(s)\n",
    "Navigate to the 'Code Objects' sidebar in the user interface of the Rhino FCP and copy the UID of the 'Data Harmonization' Code Object associated with your Copilot implementation. This will be used to configure the SDK code that executes the ETL process. \n",
    "\n",
    "<img src=images/copy_uid.png alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f3e593e-4bbc-4c3d-a995-7cdbd0e97ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: Failed to make request\n",
      "Status is 400, Error: , Content is b'{\"errors\":[{\"title\":\"Validation Error\",\"message\":\"Running code objects of type \\'Data Harmonization\\' is not supported\",\"extra_info\":{}}]}'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# configure Data Harmonization Code Run\n",
    "code_object_params = CodeObjectRunInput(\n",
    "code_object_uid = '06955fb9-5ee1-4c1f-ad3c-e6e66d40fc0b', # REPLACE\n",
    "input_dataset_uids = [['448dd222-d90d-47a4-8eb1-d1381cd10e57']],  # REPLACE\n",
    "output_dataset_naming_templates=['{{ input_dataset_names.0 }}-out'],\n",
    "timeout_seconds=300,#REPLACE\n",
    ")\n",
    "\n",
    "# run Python code object\n",
    "code_run = session.code_object.run_code_object(code_object_params)\n",
    "run_result = code_run.wait_for_completion()\n",
    "#harmonized_dataset = run_result.output_dataset\n",
    "print(f\"Result status is '{run_result.status.value}', errors={run_result.results_info.get('errors') if run_result.results_info else None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f07c71-9521-462f-9a75-1cdb76de18d5",
   "metadata": {},
   "source": [
    "##### Export dataset to your Rhino client\n",
    "Once the datasets are created, you can export them as CSVs onto your Rhino client (aka server) and these CSVs can be automatically loaded back into your enterprise data warehouse or send to research collaborators or reporting agencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ade3da50-67ac-4b46-bf91-dc75068f59ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: Failed to make request\n",
      "Status is 400, Error: , Content is b'{\"errors\":[{\"title\":\"Validation error\",\"message\":\"output_location: Input should be a valid string\\\\noutput_format: Input should be \\'csv\\' or \\'json\\'\",\"extra_info\":{}}]}'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.dataset.export_dataset(\n",
    "    dataset_uid= '448dd222-d90d-47a4-8eb1-d1381cd10e57', \n",
    "    output_location = \"test\", \n",
    "    output_format = \"CSV\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
