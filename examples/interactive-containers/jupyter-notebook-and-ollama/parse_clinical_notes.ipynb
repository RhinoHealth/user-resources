{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma NER for Diagnosis Extraction (Interactive Notebook)\n",
    "\n",
    "This notebook demonstrates how to use a Gemma instruction-tuned model (via Hugging Face `transformers`) to extract diagnosis names from clinical text based on a specific prompt.\n",
    "\n",
    "**Steps:**\n",
    "1.  Install necessary libraries.\n",
    "2.  Configure model and file paths.\n",
    "3.  Load datasets\n",
    "4.  Define the helper function for parsing model output.\n",
    "5.  Load the tokenizer and model (this may take time and resources).\n",
    "6.  Process each note: construct prompt, generate text, parse diagnosis.\n",
    "7.  Display and save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers torch pandas accelerate sentencepiece bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = '/Users/adrish/Desktop/ehrcon-dataset/workgroup-notes.csv'\n",
    "SELECTED_COLUMNS = ['SUBJECT_ID','TEXT'] \n",
    "OUTPUT_FILE = 'extracted_diagnoses_v2.csv'\n",
    "# Choose model: 'google/gemma-3-1b-it' (faster, less VRAM) or 'google/gemma-7b-it' (potentially better, more VRAM)\n",
    "MODEL_NAME = 'google/gemma-3-1b-it' \n",
    "\n",
    "# Optional: Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Helper Function for Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to clean model output ---\n",
    "def parse_diagnoses(model_output, prompt_text):\n",
    "    \"\"\"\n",
    "    Parses the model's raw output to extract the comma-separated diagnoses.\n",
    "    Removes the input prompt part and cleans the result.\n",
    "    \"\"\"\n",
    "    # Find the start of the actual answer (after the prompt)\n",
    "    # Look for the marker we put at the end of the prompt\n",
    "    answer_marker = \"Extracted Diagnoses (comma-separated):\"\n",
    "    try:\n",
    "        # Find the position *after* the marker\n",
    "        start_index = model_output.index(answer_marker) + len(answer_marker)\n",
    "        # Extract the text after the marker\n",
    "        extracted_part = model_output[start_index:].strip()\n",
    "\n",
    "        # Sometimes models add extra text or explanations after the list.\n",
    "        # Try to find the first newline character after the start, assuming the list is on one line.\n",
    "        first_newline = extracted_part.find('\\n')\n",
    "        if first_newline != -1:\n",
    "            extracted_part = extracted_part[:first_newline].strip()\n",
    "\n",
    "        # Remove potential trailing tags or symbols often added by models\n",
    "        extracted_part = re.sub(r'<eos>$|</s>$', '', extracted_part).strip() # Remove end-of-sequence tokens if present\n",
    "\n",
    "        # Check if the model explicitly said \"None\"\n",
    "        if extracted_part.lower() == 'none':\n",
    "            return []\n",
    "\n",
    "        # Split by comma and clean up each item\n",
    "        diagnoses = [diag.strip() for diag in extracted_part.split(',') if diag.strip()]\n",
    "        \n",
    "        # Final sanity check: remove any empty strings that might remain\n",
    "        diagnoses = [d for d in diagnoses if d]\n",
    "\n",
    "        return diagnoses\n",
    "\n",
    "    except ValueError:\n",
    "        # If the marker isn't found, the model output format was unexpected.\n",
    "        logging.warning(f\"Could not find answer marker '{answer_marker}' in model output. Trying fallback. Raw output: {model_output[:500]}...\") # Log truncated output\n",
    "        # Attempt a simpler extraction based on the last line (less reliable)\n",
    "        lines = model_output.strip().split('\\n')\n",
    "        if lines:\n",
    "            last_line = lines[-1].strip()\n",
    "             # Avoid taking the prompt itself as the answer if it appears last\n",
    "            if answer_marker not in last_line and last_line.lower() != 'none':\n",
    "                 # Basic split and clean, might capture unwanted text\n",
    "                 diagnoses = [diag.strip() for diag in last_line.split(',') if diag.strip()] \n",
    "                 return [d for d in diagnoses if d] # Ensure no empty strings\n",
    "        return [] # Return empty list if parsing fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 22:23:17,404 - INFO - Loading model: google/gemma-3-1b-it\n",
      "2025-04-21 22:23:17,406 - INFO - Using device: cpu\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <2BD1B165-EC09-3F68-BCE4-8FE4E70CA7E2> /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <25C510F7-7AEE-3D64-80ED-95874DC6BECD> /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-04-21 22:23:27,776 - WARNING - Some parameters are on the meta device because they were offloaded to the disk.\n",
      "2025-04-21 22:23:27,782 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Loading model: {MODEL_NAME}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # Ensure pad_token is set if missing (common issue with some models)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        logging.info(\"Set tokenizer pad_token to eos_token\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\", # Automatically use GPU if available\n",
    "        torch_dtype=torch.bfloat16 # Use bfloat16 for faster inference if supported, requires Ampere GPU or newer\n",
    "        # torch_dtype=torch.float16 # Alternative if bfloat16 not supported\n",
    "    )\n",
    "    logging.info(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model: {e}\")\n",
    "    # Optionally, raise the error to stop execution if model loading fails\n",
    "    # raise e "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Input Data\n",
    "\n",
    "Load the clinical notes from the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 22:23:32,276 - INFO - Loading input data from: /Users/adrish/Desktop/ehrcon-dataset/workgroup-notes.csv\n",
      "2025-04-21 22:23:32,322 - INFO - Loaded 9 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>\"Patient presents with symptoms indicative of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>89 yo M with a history of prostate CA and Alzh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>52 yo male with Down's syndrome and NAFLD who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400</td>\n",
       "      <td>Patient is a 50yo woman with adenoid cystic ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>The patient is a 78-year-old woman with a hist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID                                               TEXT\n",
       "0         100  \"Patient presents with symptoms indicative of ...\n",
       "1         200  89 yo M with a history of prostate CA and Alzh...\n",
       "2         300  52 yo male with Down's syndrome and NAFLD who ...\n",
       "3         400  Patient is a 50yo woman with adenoid cystic ca...\n",
       "4         500  The patient is a 78-year-old woman with a hist..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.info(f\"Loading input data from: {INPUT_FILE}\")\n",
    "try:\n",
    "    df_input = pd.read_csv(INPUT_FILE)\n",
    "    df_input = df_input[SELECTED_COLUMNS]\n",
    "    # Verify required columns exist\n",
    "    if \"SUBJECT_ID\" not in df_input.columns or \"TEXT\" not in df_input.columns:\n",
    "        raise ValueError(\"Input CSV must contain 'subject id' and 'text' columns.\")\n",
    "    # Handle potential missing text data\n",
    "    df_input['TEXT'] = df_input['TEXT'].fillna('')\n",
    "    logging.info(f\"Loaded {len(df_input)} records.\")\n",
    "    print(\"Input Data Sample:\")\n",
    "    display(df_input.head()) # Display first 5 rows in Jupyter\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Input file not found: {INPUT_FILE}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error reading input CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process Records\n",
    "\n",
    "Iterate through each record, generate the diagnosis using the model, parse the output, and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing record 1/9 for patient ID: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found diagnoses: ['Pneumonia', 'Hypertension', 'Type 2 Diabetes']\n",
      "\n",
      "Processing record 2/9 for patient ID: 200\n",
      "  Found diagnoses: ['[Hospital1 18]', \"Alzheimer's dementia\", 'Prostate CA', 'Maroon', 'guaiac positive stool', 'Gerontology']\n",
      "\n",
      "Processing record 3/9 for patient ID: 300\n",
      "  Found diagnoses: [\"Down's syndrome\", 'NAFLD', 'facial rash', 'petechial rash', 'fevers']\n",
      "\n",
      "Processing record 4/9 for patient ID: 400\n",
      "  Found diagnoses: ['Adenoid cystic carcinoma', 'pneumonectomy', 'liver', 'kidney', 'PE', 'fevers', 'lethargy', 'pleuritic CP']\n",
      "\n",
      "Processing record 5/9 for patient ID: 500\n",
      "  Found diagnoses: ['Doctor Last Name 688', 'Diabetes', 'Carotid stenosis', 'Chronic back pain', 'Lethargy', 'Facial droop', 'Voice weakness', 'Slurred speech', 'Cough', 'Abdominal pain', 'Diarrhea', 'Constipation', 'Fevers', 'Chills', 'Chest pain', 'Shortness of breath']\n",
      "\n",
      "Processing record 6/9 for patient ID: 600\n",
      "  Found diagnoses: ['Last Name', 'Location', 'Acute abdominal back pain', 'systolic blood pressure', 'hematocrit', 'IV access', 'Foley', 'Vascular Surgery', 'emergency room', 'helicopter pad', 'ischemic changes', 'packed red blood cells.']\n",
      "\n",
      "Processing record 7/9 for patient ID: 700\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "total_records = len(df_input)\n",
    "start_time = time.time()\n",
    "\n",
    "for index, row in df_input.iterrows():\n",
    "    patient_id = row['SUBJECT_ID']\n",
    "    clinical_text = row['TEXT']\n",
    "\n",
    "    print(f\"\\nProcessing record {index + 1}/{total_records} for patient ID: {patient_id}\")\n",
    "\n",
    "    if not clinical_text or pd.isna(clinical_text) or not clinical_text.strip():\n",
    "         logging.warning(f\"  Skipping record {index + 1} due to empty clinical text.\")\n",
    "         print(\"  Skipping due to empty text.\")\n",
    "         continue # Skip records with no text\n",
    "\n",
    "    # --- Construct the Prompt ---\n",
    "    prompt = f\"\"\"Objective: Identify all diagnosis names mentioned in the following clinical text.\n",
    "Guidelines:\n",
    "1. Extract only diagnosis names.\n",
    "2. Extract the entity exactly as written in the note without modification.\n",
    "3. Only extract diagnoses explicitly listed in the text. Do not infer or add conditions not present.\n",
    "4. Ignore numeric values unless they are part of a specific diagnosis name (e.g., 'Type 2 Diabetes', 'stage 3').\n",
    "5. Focus on conditions, diseases, syndromes, and specific medical problems mentioned.\n",
    "6. Output Format: List the extracted diagnosis names separated by commas. If no diagnoses are found, output \"None\".\n",
    "\n",
    "Clinical Text:\n",
    "---\n",
    "{clinical_text}\n",
    "---\n",
    "\n",
    "Extracted Diagnoses (comma-separated):\"\"\"\n",
    "\n",
    "    # --- Generate Text with the Model ---\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=10000).to(model.device) # Adjust max_length if needed\n",
    "        # Adjust generation parameters as needed\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10000,  # Max tokens for the *answer* part\n",
    "            do_sample=False,     # Use greedy decoding for consistency\n",
    "            temperature=0,   # Optional: for slight randomness if needed\n",
    "            # top_k=50,          # Optional: sampling parameters\n",
    "            pad_token_id=tokenizer.pad_token_id # Ensure padding token is set\n",
    "            )\n",
    "        # Decode the full output (including the prompt part)\n",
    "        full_output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # --- Parse the Output ---\n",
    "        # print(f\"\\nDEBUG: Raw model output:\\n{full_output_text}\\n---\\n\") # Uncomment for debugging raw output\n",
    "        extracted_diagnoses = parse_diagnoses(full_output_text, prompt) # Pass the original prompt text\n",
    "\n",
    "        # --- Store Results ---\n",
    "        if extracted_diagnoses:\n",
    "            print(f\"  Found diagnoses: {extracted_diagnoses}\")\n",
    "            for diagnosis in extracted_diagnoses:\n",
    "                results.append({'patient_id': patient_id, 'entity_name': diagnosis})\n",
    "        else:\n",
    "            print(f\"  No diagnoses found.\")\n",
    "            # If you want to explicitly record patients with no diagnoses found:\n",
    "            # results.append({'patient id': patient_id, 'entity_name': 'None'}) # Optional based on requirements\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing record {index + 1} for patient ID {patient_id}: {e}\")\n",
    "        print(f\"  Error processing record: {e}\")\n",
    "        # Optionally add a placeholder for failed records\n",
    "        # results.append({'patient id': patient_id, 'entity_name': 'Processing_Error'})\n",
    "    \n",
    "    # Small delay to potentially help with GPU cooling or API rate limits if applicable\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "print(f\"\\n-------------------------------------------------\")\n",
    "logging.info(f\"Finished processing all records. Total time: {processing_time:.2f} seconds.\")\n",
    "print(f\"Finished processing all records. Total time: {processing_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Display and Save Results\n",
    "\n",
    "Convert the collected results into a DataFrame, display the first few rows, and save the complete list to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Diagnoses Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient id</th>\n",
       "      <th>entity_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4954</td>\n",
       "      <td>on [**2152-5-22**].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13342</td>\n",
       "      <td>intermittent mild grunting respirations.  The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66296</td>\n",
       "      <td>[**2141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>Prematurity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>Triplet #3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient id                                    entity_name\n",
       "0        4954                            on [**2152-5-22**].\n",
       "1       13342  intermittent mild grunting respirations.  The\n",
       "2       66296                                        [**2141\n",
       "3          60                                    Prematurity\n",
       "4          60                                     Triplet #3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 21:20:41,419 - INFO - Saving 10 extracted diagnosis entries to: extracted_diagnoses_v1.csv\n",
      "2025-04-21 21:20:41,427 - INFO - Output file saved successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output saved successfully to extracted_diagnoses_v1.csv\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    df_output = pd.DataFrame(results)\n",
    "    print(\"\\nExtracted Diagnoses Sample:\")\n",
    "    display(df_output.head()) # Display first 5 rows\n",
    "    \n",
    "    logging.info(f\"Saving {len(df_output)} extracted diagnosis entries to: {OUTPUT_FILE}\")\n",
    "    try:\n",
    "        df_output.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"\\nOutput saved successfully to {OUTPUT_FILE}\")\n",
    "        logging.info(\"Output file saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving output file: {e}\")\n",
    "        print(f\"\\nError saving output file: {e}\")\n",
    "else:\n",
    "    logging.info(\"No diagnoses were extracted from any record.\")\n",
    "    print(\"\\nNo diagnoses were extracted from any record.\")\n",
    "    # Optionally create an empty file or a file indicating no results\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, 'w') as f:\n",
    "             f.write(\"patient id,entity_name\\n\") # Create header for empty file\n",
    "        logging.info(f\"Empty output file created: {OUTPUT_FILE}\")\n",
    "        print(f\"Empty output file created: {OUTPUT_FILE}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating empty output file: {e}\")\n",
    "        print(f\"Error creating empty output file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
